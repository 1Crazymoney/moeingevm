package ebp

import (
	"encoding/binary"
	"sync"
	"sync/atomic"

	"github.com/ethereum/go-ethereum/common"
	gethtypes "github.com/ethereum/go-ethereum/core/types"
	"github.com/holiman/uint256"
	"github.com/smartbch/MoeingADS/store/rabbit"

	"github.com/smartbch/MoeingEVM/types"
	"github.com/smartbch/MoeingEVM/utils"
)

var _ TxExecutor = (*txEngine)(nil)

type TxRange struct {
	start uint64
	end   uint64
}

type txEngine struct {
	// How many parallel execution round are performed for each block
	roundNum int //consensus parameter
	// How many runners execute transactions in parallel for each round
	runnerNumber int //consensus parameter
	// The runners are driven by 'parallelNum' of goroutines
	parallelNum int //per-node parameter
	// A clean Context whose RabbitStore has no cache. It must be set before calling 'Execute', and
	// txEngine will close it at the end of 'Prepare'
	cleanCtx *types.Context
	// CollectTx fills txList and 'Prepare' handles and clears txList
	txList       []*gethtypes.Transaction
	committedTxs []*types.Transaction
	// Used to check signatures
	signer       gethtypes.Signer
	currentBlock *types.BlockInfo

	cumulativeGasUsed uint64
}

func (exec *txEngine) Context() *types.Context {
	return exec.cleanCtx
}

// Generated by parallelReadAccounts and insertToStandbyTxQ will store its tx into world state.
type preparedInfo struct {
	tx        *types.TxToRun
	gasFee    uint256.Int
	valid     bool
	statusStr string
}

// Generated by parallelReadAccounts and Prepare will use them for some tests.
type ctxAndAccounts struct {
	ctx      types.Context
	accounts []common.Address
	nonces   []uint64
	changed  bool
}

func NewEbpTxExec(exeRoundCount, runnerNumber, parallelNum, defaultTxListCap int, s gethtypes.Signer) *txEngine {
	Runners = make([]*TxRunner, runnerNumber)
	return &txEngine{
		roundNum:     exeRoundCount,
		runnerNumber: runnerNumber,
		parallelNum:  parallelNum,
		txList:       make([]*gethtypes.Transaction, 0, defaultTxListCap),
		committedTxs: make([]*types.Transaction, 0, defaultTxListCap),
		signer:       s,
	}
}

// A new context must be set before Execute
func (exec *txEngine) SetContext(ctx *types.Context) {
	exec.cleanCtx = ctx
}

// Check transactions' signatures and insert the valid ones into standby queue
func (exec *txEngine) Prepare() {
	if len(exec.txList) == 0 {
		exec.cleanCtx.Close(false)
		return
	}
	infoList, ctxAA := exec.parallelReadAccounts()
	addr2idx := make(map[common.Address]int)      // map address to ctxAA's index
	addr2nonce := make(map[common.Address]uint64) // record each address's nonce
	for idx, entry := range ctxAA {
		for j, addr := range entry.accounts {
			if _, ok := addr2idx[addr]; !ok {
				//when we meet an address for the first time
				addr2idx[addr] = idx
				addr2nonce[addr] = entry.nonces[j]
			}
		}
	}
	for i := range infoList {
		if !infoList[i].valid {
			//skip it if account does not exist or signature is invalid
			continue
		}
		sender := infoList[i].tx.From
		if addr2nonce[sender] != infoList[i].tx.Nonce {
			//skip it if nonce is wrong
			infoList[i].valid = false
			infoList[i].statusStr = "incorrect nonce"
			continue
		}
		addr2nonce[sender]++
		idx := addr2idx[sender]
		acc := ctxAA[idx].ctx.GetAccount(sender) // read cached account
		x := acc.Balance()
		x.Sub(x, &infoList[i].gasFee)
		if x.Cmp(acc.Balance()) == 1 {
			//skip it if balance cannot pay gasfee
			infoList[i].valid = false
			infoList[i].statusStr = "not enough balance to pay gasfee"
			continue
		}
		acc.UpdateBalance(x)
		ctxAA[idx].ctx.SetAccount(sender, acc)
		ctxAA[idx].changed = true //now this rabbit needs writeback
	}
	for i := range ctxAA {
		ctxAA[i].ctx.Close(ctxAA[i].changed)
	}
	// the value of exec.parallelNum and the speeds of goroutines must have
	// no effects on the order of TXs in standby queue.
	ctx := exec.cleanCtx.WithRbtCopy()
	exec.insertToStandbyTxQ(ctx, infoList)
	exec.txList = exec.txList[:0] // clear txList after consumption
	//write ctx state to trunk
	ctx.Close(true)
	exec.cleanCtx.Close(false)
}

// Read accounts' information in parallel, while checking accouts' existence and signatures' validity
func (exec *txEngine) parallelReadAccounts() (infoList []preparedInfo, ctxAA []ctxAndAccounts) {
	//for each tx, we fetch some info for it
	infoList = make([]preparedInfo, len(exec.txList))
	//the ctx and accounts that a worker works at
	ctxAA = make([]ctxAndAccounts, exec.parallelNum)
	sharedIdx := int64(-1)
	estimatedSize := len(exec.txList)/exec.parallelNum + 1
	parallelRun(exec.parallelNum, func(workerId int) {
		ctxAA[workerId].ctx = *exec.cleanCtx.WithRbtCopy()
		ctxAA[workerId].accounts = make([]common.Address, 0, estimatedSize)
		ctxAA[workerId].nonces = make([]uint64, 0, estimatedSize)
		ctxAA[workerId].changed = false
		for {
			myIdx := atomic.AddInt64(&sharedIdx, 1)
			if int(myIdx) >= len(exec.txList) {
				return
			}
			tx := exec.txList[myIdx]
			sender, err := exec.signer.Sender(tx)
			if err != nil {
				infoList[myIdx].valid = false
				infoList[myIdx].statusStr = "invalid signature"
				continue // skip invalid signature
			}
			txToRun := &types.TxToRun{}
			txToRun.FromGethTx(tx, sender, exec.cleanCtx.Height)
			acc := ctxAA[workerId].ctx.GetAccount(sender)
			infoList[myIdx].valid = acc != nil
			if acc == nil {
				infoList[myIdx].statusStr = "non-existent account"
				continue // skip non-existent account
			}
			ctxAA[workerId].accounts = append(ctxAA[workerId].accounts, sender)
			ctxAA[workerId].nonces = append(ctxAA[workerId].nonces, acc.Nonce())
			infoList[myIdx].tx = txToRun
			infoList[myIdx].gasFee.Mul(uint256.NewInt().SetUint64(txToRun.Gas),
				utils.U256FromSlice32(txToRun.GasPrice[:]))
		}
	})
	return
}

// insert valid transactions into standby queue
func (exec *txEngine) insertToStandbyTxQ(ctx *types.Context, infoList []preparedInfo) {
	rbt := ctx.Rbt
	startEnd := rbt.Get(types.StandbyTxQueueKey)
	end := uint64(0)
	if len(startEnd) >= 8 {
		end = binary.BigEndian.Uint64(startEnd[8:])
	} else {
		startEnd = make([]byte, 16)
	}
	for _, info := range infoList {
		if !info.valid {
			exec.recordInvalidTx(info)
			continue
		}
		k := types.GetStandbyTxKey(end)
		rbt.Set(k, info.tx.ToBytes())
		end++
	}
	binary.BigEndian.PutUint64(startEnd[8:], end)
	rbt.Set(types.StandbyTxQueueKey, startEnd) //update start&end pointers of standby queue
}

func (exec *txEngine) recordInvalidTx(info preparedInfo) {
	tx := &types.Transaction{
		Hash:              info.tx.HashID,
		TransactionIndex:  int64(len(exec.committedTxs)),
		Nonce:             info.tx.Nonce,
		BlockNumber:       int64(exec.cleanCtx.Height),
		From:              info.tx.From,
		To:                info.tx.To,
		Value:             info.tx.Value,
		GasPrice:          info.tx.GasPrice,
		Gas:               info.tx.Gas,
		Input:             info.tx.Data,
		CumulativeGasUsed: exec.cumulativeGasUsed,
		GasUsed:           0,
		Status:            gethtypes.ReceiptStatusFailed,
		StatusStr:         info.statusStr,
	}
	if exec.currentBlock != nil {
		tx.BlockHash = exec.currentBlock.Hash
	}
	exec.committedTxs = append(exec.committedTxs, tx)
}

// Fetch TXs from standby queue and execute them
func (exec *txEngine) Execute(currBlock *types.BlockInfo) {
	exec.committedTxs = exec.committedTxs[:0]
	exec.currentBlock = currBlock
	startKey, endKey := exec.getStandbyQueueRange()
	if startKey == endKey {
		//fmt.Println("::::DEBUG: no transaction to execute in ExecuteNRound")
		return
	}
	txRange := &TxRange{
		start: startKey,
		end:   endKey,
	}
	committableTxList := make([]*TxRunner, 0, 4096)
	// Repeat exec.roundNum round for execute txs in standby q. At the end of each round
	// modifications made by TXs are written to world state. So TXs in later rounds will
	// be affected by the modifications made by TXs in earlier rounds.
	for i := 0; i < exec.roundNum; i++ {
		if txRange.start == txRange.end {
			break
		}
		numTx := exec.executeOneRound(txRange, exec.currentBlock)
		//var tmp = make([]*TxRunner, 0, 4096)
		for i := 0; i < numTx; i++ {
			if Runners[i] == nil {
				continue
			}
			committableTxList = append(committableTxList, Runners[i])
			//tmp = append(tmp, Runners[i])
			Runners[i] = nil
		}
		//for _, runner := range tmp {
		//			fmt.Printf(`
		//from:%s
		//to:%s
		//nonce:%d
		//value :%d
		//`, runner.Tx.From.String(), runner.Tx.To.String(), runner.Tx.Nonce, runner.Tx.Value)
		//}
	}
	exec.setStandbyQueueRange(txRange.start, txRange.end)
	exec.collectCommittableTxs(committableTxList)
	return
}

// Get the start and end position of standby queue
func (exec *txEngine) getStandbyQueueRange() (start, end uint64) {
	ctx := exec.cleanCtx.WithRbtCopy()
	defer ctx.Close(false)
	startEnd := ctx.Rbt.Get(types.StandbyTxQueueKey)
	if startEnd == nil {
		return 0, 0
	}
	return binary.BigEndian.Uint64(startEnd[:8]), binary.BigEndian.Uint64(startEnd[8:])
}

// Set the start and end position of standby queue
func (exec *txEngine) setStandbyQueueRange(start, end uint64) {
	ctx := exec.cleanCtx.WithRbtCopy()
	startEnd := make([]byte, 16)
	binary.BigEndian.PutUint64(startEnd[:8], start)
	binary.BigEndian.PutUint64(startEnd[8:], end)
	ctx.Rbt.Set(types.StandbyTxQueueKey, startEnd)
	ctx.Close(true)
}

// Execute 'runnerNumber' transactions in parallel and commit the ones without any interdependency
func (exec *txEngine) executeOneRound(txRange *TxRange, currBlock *types.BlockInfo) int {
	standbyTxList := exec.loadStandbyTxs(txRange)
	exec.runTxInParallel(standbyTxList, currBlock)
	exec.checkTxDepsAndUptStandbyQ(txRange, standbyTxList)
	return len(standbyTxList)
}

// Load at most 'exec.runnerNumber' transactions from standby queue
func (exec *txEngine) loadStandbyTxs(txRange *TxRange) (txBundle []types.TxToRun) {
	ctx := exec.cleanCtx.WithRbtCopy()
	end := txRange.end
	if end > txRange.start+uint64(exec.runnerNumber) {
		end = txRange.start + uint64(exec.runnerNumber)
	}
	txBundle = make([]types.TxToRun, end-txRange.start)
	for i := txRange.start; i < end; i++ {
		k := types.GetStandbyTxKey(i)
		bz := ctx.Rbt.Get(k)
		txBundle[i-txRange.start].FromBytes(bz)
	}
	ctx.Close(false)
	return
}

// Assign the transactions to global 'Runners' and run them in parallel
func (exec *txEngine) runTxInParallel(txBundle []types.TxToRun, currBlock *types.BlockInfo) {
	sharedIdx := int64(-1)
	parallelRun(exec.parallelNum, func(_ int) {
		for {
			myIdx := atomic.AddInt64(&sharedIdx, 1)
			if myIdx >= int64(len(txBundle)) {
				return
			}
			Runners[myIdx] = &TxRunner{
				id:  myIdx,
				Ctx: *exec.cleanCtx.WithRbtCopy(),
				Tx:  &txBundle[myIdx],
			}
			runTx(int(myIdx), currBlock)
		}
	})
}

// Check interdependency of TXs using 'touchedSet'. The ones with dependency with former committed TXs cannot
// be committed and should be inserted back into the standby queue.
// A TX whose nonce is too small should also be inserted back into the standby queue.
func (exec *txEngine) checkTxDepsAndUptStandbyQ(txRange *TxRange, standbyTxList []types.TxToRun) {
	touchedSet := make(map[uint64]struct{}, 1000)
	for idx := range standbyTxList {
		canCommit := true
		Runners[idx].Ctx.Rbt.ScanAllShortKeys(func(key [rabbit.KeySize]byte, dirty bool) (stop bool) {
			k := binary.LittleEndian.Uint64(key[:])
			if _, ok := touchedSet[k]; ok {
				canCommit = false // cannot commit if conflicts with touched KV set
				Runners[idx].Status = types.FAILED_TO_COMMIT
				return true
			} else {
				return false
			}
		})
		if canCommit { // record the dirty KVs written by a committable TX into toucchedSet
			Runners[idx].Ctx.Rbt.ScanAllShortKeys(func(key [rabbit.KeySize]byte, dirty bool) (stop bool) {
				if dirty {
					k := binary.LittleEndian.Uint64(key[:])
					touchedSet[k] = struct{}{}
				}
				return false
			})
		}
		Runners[idx].Ctx.Rbt.CloseAndWriteBack(canCommit)
	}

	ctx := exec.cleanCtx.WithRbtCopy()
	for idx, tx := range standbyTxList {
		k := types.GetStandbyTxKey(txRange.start)
		txRange.start++
		ctx.Rbt.Delete(k) // remove it from the standby queue
		status := Runners[idx].Status
		if status == types.FAILED_TO_COMMIT || status == types.TX_NONCE_TOO_LARGE {
			newK := types.GetStandbyTxKey(txRange.end)
			txRange.end++
			ctx.Rbt.Set(newK, tx.ToBytes()) // insert the failed TXs back into standby queue
			Runners[idx] = nil
		} else if status == types.ACCOUNT_NOT_EXIST || status == types.TX_NONCE_TOO_SMALL {
			Runners[idx] = nil
		}
	}
	ctx.Close(true)
}

// Fill 'exec.committedTxs' with 'committableTxList'
func (exec *txEngine) collectCommittableTxs(committableTxList []*TxRunner) {
	exec.cumulativeGasUsed = 0
	var logIndex uint
	for idx, runner := range committableTxList {
		exec.cumulativeGasUsed += runner.GasUsed
		tx := &types.Transaction{
			Hash:              runner.Tx.HashID,
			TransactionIndex:  int64(idx),
			Nonce:             runner.Tx.Nonce,
			BlockHash:         exec.currentBlock.Hash,
			BlockNumber:       int64(exec.cleanCtx.Height),
			From:              runner.Tx.From,
			To:                runner.Tx.To,
			Value:             runner.Tx.Value,
			GasPrice:          runner.Tx.GasPrice,
			Gas:               runner.Tx.Gas,
			Input:             runner.Tx.Data,
			CumulativeGasUsed: exec.cumulativeGasUsed,
			GasUsed:           runner.GasUsed,
			ContractAddress:   runner.CreatedContractAddress, //20 Bytes - the contract address created, if the transaction was a contract creation, otherwise - null. TODO testme!
			OutData:           append([]byte{}, runner.OutData...),
			Status:            gethtypes.ReceiptStatusSuccessful,
			StatusStr:         StatusToStr(runner.Status),
		}
		if StatusIsFailure(runner.Status) {
			tx.Status = gethtypes.ReceiptStatusFailed
		}
		//fmt.Println(tx.ContractAddress)
		tx.Logs = make([]types.Log, len(runner.Logs))
		for i, log := range runner.Logs {
			copy(tx.Logs[i].Address[:], log.Address[:])
			tx.Logs[i].Topics = make([][32]byte, len(log.Topics))
			for j, t := range log.Topics {
				copy(tx.Logs[i].Topics[j][:], t[:])
			}
			tx.Logs[i].Data = log.Data
			log.Data = nil
			tx.Logs[i].BlockNumber = uint64(exec.currentBlock.Number)
			copy(tx.Logs[i].BlockHash[:], exec.currentBlock.Hash[:])
			copy(tx.Logs[i].TxHash[:], tx.Hash[:])
			//txIndex = index in committableTxList
			tx.Logs[i].TxIndex = uint(idx)
			tx.Logs[i].Index = logIndex
			logIndex++
			tx.Logs[i].Removed = false
		}
		tx.LogsBloom = LogsBloom(tx.Logs)
		exec.committedTxs = append(exec.committedTxs, tx)
	}
}

func LogsBloom(logs []types.Log) [256]byte {
	var bin gethtypes.Bloom
	for _, log := range logs {
		bin.Add(log.Address[:])
		for _, b := range log.Topics {
			bin.Add(b[:])
		}
	}
	return bin
}

func (exec *txEngine) CommittedTxs() []*types.Transaction {
	return exec.committedTxs
}

func (exec *txEngine) CollectTx(tx *gethtypes.Transaction) {
	exec.txList = append(exec.txList, tx)
}

func (exec *txEngine) CollectTxsCount() int {
	return len(exec.txList)
}

func parallelRun(workerCount int, fn func(workerID int)) {
	var wg sync.WaitGroup
	wg.Add(workerCount)
	for i := 0; i < workerCount; i++ {
		go func(i int) {
			fn(i)
			wg.Done()
		}(i)
	}
	wg.Wait()
}
